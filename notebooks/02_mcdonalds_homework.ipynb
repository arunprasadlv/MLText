{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework with McDonald's sentiment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaginary problem statement\n",
    "\n",
    "McDonald's receives **thousands of customer comments** on their website per day, and many of them are negative. Their corporate employees don't have time to read every single comment, but they do want to read a subset of comments that they are most interested in. In particular, the media has recently portrayed their employees as being rude, and so they want to review comments about **rude service**.\n",
    "\n",
    "McDonald's has hired you to develop a system that ranks each comment by the **likelihood that it is referring to rude service**. They will use your system to build a \"rudeness dashboard\" for their corporate employees, so that employees can spend a few minutes each day examining the **most relevant recent comments**.\n",
    "\n",
    "## Description of the data\n",
    "\n",
    "Before hiring you, McDonald's used the [CrowdFlower platform](http://www.crowdflower.com/data-for-everyone) to pay humans to **hand-annotate** about 1500 comments with the **type of complaint**. The complaint types are listed below, with the encoding used in the data listed in parentheses:\n",
    "\n",
    "- Bad Food (BadFood)\n",
    "- Bad Neighborhood (ScaryMcDs)\n",
    "- Cost (Cost)\n",
    "- Dirty Location (Filthy)\n",
    "- Missing Item (MissingFood)\n",
    "- Problem with Order (OrderProblem)\n",
    "- Rude Service (RudeService)\n",
    "- Slow Service (SlowService)\n",
    "- None of the above (na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Read **`mcdonalds.csv`** into a pandas DataFrame and examine it. (It can be found in the **`data`** directory of the course repository.)\n",
    "\n",
    "- The **policies_violated** column lists the type of complaint. If there is more than one type, the types are separated by newline characters.\n",
    "- The **policies_violated:confidence** column lists CrowdFlower's confidence in the judgments of its human annotators for that row (higher is better).\n",
    "- The **city** column is the McDonald's location.\n",
    "- The **review** column is the actual text comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../data/mcdonalds.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>policies_violated</th>\n",
       "      <th>policies_violated:confidence</th>\n",
       "      <th>city</th>\n",
       "      <th>policies_violated_gold</th>\n",
       "      <th>review</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:36</td>\n",
       "      <td>RudeService\\r\\nOrderProblem\\r\\nFilthy</td>\n",
       "      <td>1.0\\r\\n0.6667\\r\\n0.6667</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:27</td>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Terrible customer service. ŒæI came in at 9:30...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:26</td>\n",
       "      <td>SlowService\\r\\nOrderProblem</td>\n",
       "      <td>1.0\\r\\n1.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455656</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:27</td>\n",
       "      <td>na</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I see I'm not the only one giving 1 star. Only...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455657</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:27</td>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Well, it's McDonald's, so you know what the fo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  679455653   False   finalized                   3      2/21/15 0:36   \n",
       "1  679455654   False   finalized                   3      2/21/15 0:27   \n",
       "2  679455655   False   finalized                   3      2/21/15 0:26   \n",
       "3  679455656   False   finalized                   3      2/21/15 0:27   \n",
       "4  679455657   False   finalized                   3      2/21/15 0:27   \n",
       "\n",
       "                       policies_violated policies_violated:confidence  \\\n",
       "0  RudeService\\r\\nOrderProblem\\r\\nFilthy      1.0\\r\\n0.6667\\r\\n0.6667   \n",
       "1                            RudeService                            1   \n",
       "2            SlowService\\r\\nOrderProblem                   1.0\\r\\n1.0   \n",
       "3                                     na                       0.6667   \n",
       "4                            RudeService                            1   \n",
       "\n",
       "      city  policies_violated_gold  \\\n",
       "0  Atlanta                     NaN   \n",
       "1  Atlanta                     NaN   \n",
       "2  Atlanta                     NaN   \n",
       "3  Atlanta                     NaN   \n",
       "4  Atlanta                     NaN   \n",
       "\n",
       "                                              review  Unnamed: 10  \n",
       "0  I'm not a huge mcds lover, but I've been to be...          NaN  \n",
       "1  Terrible customer service. ŒæI came in at 9:30...          NaN  \n",
       "2  First they \"lost\" my order, actually they gave...          NaN  \n",
       "3  I see I'm not the only one giving 1 star. Only...          NaN  \n",
       "4  Well, it's McDonald's, so you know what the fo...          NaN  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1525, 11)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Remove any rows from the DataFrame in which the **policies_violated** column has a **null value**. Check the shape of the DataFrame before and after to confirm that you only removed about 50 rows.\n",
    "\n",
    "- **Note:** Null values are also known as \"missing values\", and are encoded in pandas with the special value \"NaN\". This is distinct from the \"na\" encoding used by CrowdFlower to denote \"None of the above\". Rows that contain \"na\" should **not** be removed.\n",
    "- **Hint:** [How do I handle missing values in pandas?](https://www.youtube.com/watch?v=fCMrO_VzeL8&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=16) explains how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_unit_id                           0\n",
       "_golden                            0\n",
       "_unit_state                        0\n",
       "_trusted_judgments                 0\n",
       "_last_judgment_at                  0\n",
       "policies_violated                 54\n",
       "policies_violated:confidence      54\n",
       "city                              87\n",
       "policies_violated_gold          1525\n",
       "review                             0\n",
       "Unnamed: 10                     1525\n",
       "dtype: int64"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#policies_violated,policies_violated:confidence,city and policies_violated_gold have missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471, 11)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the rows if polices_violated has any missing value(NAN)\n",
    "df.dropna(subset=['policies_violated'],how='all').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.dropna(subset=['policies_violated'],how='all',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471, 11)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Add a new column to the DataFrame called **\"rude\"** that is 1 if the **policies_violated** column contains the text \"RudeService\", and 0 if the **policies_violated** column does not contain \"RudeService\". The \"rude\" column is going to be your response variable, so check how many zeros and ones it contains.\n",
    "\n",
    "- **Hint:** [How do I use string methods in pandas?](https://www.youtube.com/watch?v=bofaC0IckHo&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=12) shows how to search for the presence of a substring, and [How do I change the data type of a pandas Series?](https://www.youtube.com/watch?v=V0AWyzVMf54&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=13) shows how to convert the boolean results (True/False) to integers (1/0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['rude']=df.policies_violated.str.contains('RudeService').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policies_violated</th>\n",
       "      <th>rude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RudeService\\r\\nOrderProblem\\r\\nFilthy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SlowService\\r\\nOrderProblem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       policies_violated  rude\n",
       "0  RudeService\\r\\nOrderProblem\\r\\nFilthy     1\n",
       "1                            RudeService     1\n",
       "2            SlowService\\r\\nOrderProblem     0\n",
       "3                                     na     0\n",
       "4                            RudeService     1"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0:4,['policies_violated', 'rude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "1. Define X (the **review** column) and y (the **rude** column).\n",
    "2. Split X and y into training and testing sets (using the parameter **`random_state=1`**).\n",
    "3. Use CountVectorizer (with the **default parameters**) to create document-term matrices from X_train and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471L,)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['review']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471L,)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['rude']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    968\n",
       "1    503\n",
       "Name: rude, dtype: int64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103L,)\n",
      "(368L,)\n"
     ]
    }
   ],
   "source": [
    "# examine the object shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit and transform X_train\n",
    "X_train_dtm = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only transform X_test\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103, 7300)\n",
      "(368, 7300)\n"
     ]
    }
   ],
   "source": [
    "# examine the shapes: rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7300\n"
     ]
    }
   ],
   "source": [
    "print(X_train_dtm.shape[1]) # of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\u0153\\xe6yay', u'\\u0153\\xe6yeah', u'\\u0153\\xe6years', u'\\u0153\\xe6yelp', u'\\u0153\\xe6yep', u'\\u0153\\xe6yes', u'\\u0153\\xe6yesterday', u'\\u0153\\xe6yet', u'\\u0153\\xe6you', u'\\u0153\\xe6your']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Fit a Multinomial Naive Bayes model to the training set, calculate the **predicted probabilites** (not the class predictions) for the testing set, and then calculate the **AUC**. Repeat this task using a logistic regression model to see which of the two models achieves a better AUC.\n",
    "\n",
    "- **Note:** Because McDonald's only cares about ranking the comments by the likelihood that they refer to rude service, **classification accuracy** is not the relevant evaluation metric. **Area Under the Curve (AUC)** is a more useful evaluation metric for this scenario, since it measures the ability of the classifier to assign higher predicted probabilities to positive instances than to negative instances.\n",
    "- **Hint:** [Evaluating a classification model](https://github.com/justmarkham/scikit-learn-videos/blob/master/09_classification_metrics.ipynb) explains how to calculate predicted probabilities and AUC, and my [blog post and video](http://www.dataschool.io/roc-curves-and-auc-explained/) explain AUC in-depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import/instantiate/fit a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the predicted probability of rude=1 for each testing set observation\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368L,)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84196471149260854"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the AUC\n",
    "from sklearn import metrics\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82339850580193941"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat this task using a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "Using either Naive Bayes or logistic regression (whichever one had a better AUC in the previous step), try **tuning CountVectorizer** using some of the techniques we learned in class. Check the testing set **AUC** after each change, and find the set of parameters that increases AUC the most.\n",
    "\n",
    "- **Hint:** It is highly recommended that you adapt the **`tokenize_test()`** function from class for this purpose, since it will allow you to iterate quickly through different sets of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the AUC\n",
    "def tokenize_test(vect):\n",
    "    \n",
    "    # create document-term matrices using the vectorizer\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    \n",
    "    # print the number of features that were generated\n",
    "    print('Features:', X_train_dtm.shape[1])\n",
    "    \n",
    "    # use Multinomial Naive Bayes to calculate predicted probabilities\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "    \n",
    "    # print the AUC\n",
    "    print('AUC:', metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features:', 7300)\n",
      "('AUC:', 0.84196471149260854)\n"
     ]
    }
   ],
   "source": [
    "#default parameter\n",
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features:', 1732)\n",
      "('AUC:', 0.86223175965665233)\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english', max_df=0.3, min_df=4)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 (Challenge)\n",
    "\n",
    "The **city** column might be predictive of the response, but we are not currently using it as a feature. Let's see whether we can increase the AUC by adding it to the model:\n",
    "\n",
    "1. Create a new DataFrame column, **review_city**, that concatenates the **review** text with the **city** text. One easy way to combine string columns in pandas is by using the [`Series.str.cat()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.cat.html) method. Make sure to use the **space character** as a separator, as well as replacing **null city values** with a reasonable string value (such as 'na').\n",
    "2. Redefine X as the **review_city** column, and re-split X and y into training and testing sets.\n",
    "3. When you run **`tokenize_test()`**, CountVectorizer will simply treat the city as an extra word in the review, and thus it will automatically be included in the model! Check to see whether it increased or decreased the AUC of your **best model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>policies_violated</th>\n",
       "      <th>policies_violated:confidence</th>\n",
       "      <th>city</th>\n",
       "      <th>policies_violated_gold</th>\n",
       "      <th>review</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>rude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:36</td>\n",
       "      <td>RudeService\\r\\nOrderProblem\\r\\nFilthy</td>\n",
       "      <td>1.0\\r\\n0.6667\\r\\n0.6667</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  679455653   False   finalized                   3      2/21/15 0:36   \n",
       "\n",
       "                       policies_violated policies_violated:confidence  \\\n",
       "0  RudeService\\r\\nOrderProblem\\r\\nFilthy      1.0\\r\\n0.6667\\r\\n0.6667   \n",
       "\n",
       "      city  policies_violated_gold  \\\n",
       "0  Atlanta                     NaN   \n",
       "\n",
       "                                              review  Unnamed: 10  rude  \n",
       "0  I'm not a huge mcds lover, but I've been to be...          NaN     1  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concatenate review and city, separated by a space, replacing nulls with 'na'\n",
    "df['review_city'] = df.review.str.cat(df.city,sep=' ',na_rep='na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not a huge mcds lover, but I've been to better ones. This is by far the worst one I've ever been too! It's filthy inside and if you get drive through they completely screw up your order every time! The staff is terribly unfriendly and nobody seems to care. Atlanta\""
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine review_city for the first row to confirm that it worked\n",
    "df.loc[0, 'review_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471L,)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#redefine X\n",
    "X=df['review_city']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471L,)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#redefine y\n",
    "\n",
    "y= df.rude\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features:', 1739)\n",
      "('AUC:', 0.86493403274519154)\n"
     ]
    }
   ],
   "source": [
    "# check whether it increased or decreased the AUC of my best model\n",
    "vect = CountVectorizer(stop_words='english', max_df=0.3, min_df=4)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 (Challenge)\n",
    "\n",
    "The **policies_violated:confidence** column may be useful, since it essentially represents a measurement of the training data quality. Let's see whether we can improve the AUC by only training the model using higher-quality rows!\n",
    "\n",
    "To accomplish this, your first sub-task is to **calculate the mean confidence score for each row**, and then store those mean scores in a new column. For example, the confidence scores for the first row are `1.0\\r\\n0.6667\\r\\n0.6667`, so you should calculate a mean of `0.7778`. Here are the suggested steps:\n",
    "\n",
    "1. Using the [`Series.str.split()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html) method, convert the **policies_violated:confidence** column into lists of one or more \"confidence scores\". Save the results as a new DataFrame column called **confidence_list**.\n",
    "2. Define a function that calculates the mean of a list of numbers, and pass that function to the [`Series.apply()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html) method of the **confidence_list** column. That will calculate the mean confidence score for each row. Save those scores in a new DataFrame column called **confidence_mean**.\n",
    "    - **Hint:** [How do I apply a function to a pandas Series or DataFrame?](https://www.youtube.com/watch?v=P_q0tkYqvSk&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=30) explains how to use the `Series.apply()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['confidence_list']=df['policies_violated:confidence'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_of_list(conf_list):\n",
    "    conf_array = np.array(conf_list,dtype=float)\n",
    "    return np.mean(conf_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['confidence_mean']=df['confidence_list'].apply(mean_of_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence_list</th>\n",
       "      <th>confidence_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0, 0.6667, 0.6667]</td>\n",
       "      <td>0.7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1]</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0, 1.0]</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.6667]</td>\n",
       "      <td>0.6667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         confidence_list  confidence_mean\n",
       "0  [1.0, 0.6667, 0.6667]           0.7778\n",
       "1                    [1]           1.0000\n",
       "2             [1.0, 1.0]           1.0000\n",
       "3               [0.6667]           0.6667"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0:3,['confidence_list','confidence_mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second sub-task is to **remove lower-quality rows from the training set**, and then repeat the model building and evaluation process. Here are the suggested steps:\n",
    "\n",
    "1. Remove all rows from X_train and y_train that have a **confidence_mean lower than 0.75**. Check their shapes before and after to confirm that you removed about 300 rows.\n",
    "2. Use the **`tokenize_test()`** function to check whether filtering the training data increased or decreased the AUC of your **best model**.\n",
    "    - **Hint:** Even though X_train and y_train are separate from the mcd DataFrame, they can still be filtered using a boolean Series generated from mcd because all three objects share the same index.\n",
    "    - **Note:** It's important that we don't remove any rows from the testing set (X_test and y_test), because the testing set should be representative of the real-world data we will encounter in the future (which will contain both high-quality and low-quality rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103L,)\n",
      "(1103L,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(799L,)\n",
      "(799L,)\n"
     ]
    }
   ],
   "source": [
    "#remove lower-quality rows from training set and define X,y\n",
    "\n",
    "X_train=X_train[df.confidence_mean >= 0.75]\n",
    "y_train=y_train[df.confidence_mean >= 0.75]\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features:', 1353)\n",
      "('AUC:', 0.84973772055317132)\n"
     ]
    }
   ],
   "source": [
    "# check whether it increased or decreased the AUC of my best model\n",
    "vect = CountVectorizer(stop_words='english', max_df=0.3, min_df=4)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9 (Challenge)\n",
    "\n",
    "New comments have been submitted to the McDonald's website, and you need to **score them with the likelihood** that they are referring to rude service.\n",
    "\n",
    "1. Before making predictions on out-of-sample data, it is important to re-train your model on all relevant data using the tuning parameters and preprocessing steps that produced the best AUC above.\n",
    "    - In other words, X should be defined using either **all rows** or **only those rows with a confidence_mean of at least 0.75**, whichever produced a better AUC above.\n",
    "    - X should refer to either the **review column** or the **review_city column**, whichever produced a better AUC above.\n",
    "    - CountVectorizer should be instantiated with the **tuning parameters** that produced the best AUC above.\n",
    "    - **`train_test_split()`** should not be used during this process.\n",
    "2. Build a document-term matrix (from X) called **X_dtm**, and examine its shape.\n",
    "3. Read the new comments stored in **`mcdonalds_new.csv`** into a DataFrame called **new_comments**, and examine it.\n",
    "4. If your model uses a **review_city** column, create that column in the new_comments DataFrame. (Otherwise, skip this step.)\n",
    "5. Build a document_term matrix (from the **new_comments** DataFrame) called **new_dtm**, and examine its shape.\n",
    "6. Train your best model (Naive Bayes or logistic regression) using **X_dtm** and **y**.\n",
    "7. Predict the \"rude probability\" for each comment in **new_dtm**, and store the probabilities in an object called **new_pred_prob**.\n",
    "8. Print the **full text** for each new comment alongside its **\"rude probability\"**. (You may need to [increase the max_colwidth](https://www.youtube.com/watch?v=yiO43TQ4xvc&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=28) to see the full text.) Examine the results, and comment on how well you think the model performed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
